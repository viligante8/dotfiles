return {
  "yetone/avante.nvim",
  -- Build native libs so templates/tokenizers are available
  build = vim.fn.has("win32") ~= 0 and nil or "make",
  -- Disabled by default via .disabled filename
  dependencies = {
    "nvim-lua/plenary.nvim",
    "MunifTanjim/nui.nvim",
    "stevearc/dressing.nvim",
    "nvim-tree/nvim-web-devicons",
  },
  opts = {
    -- Updated to use `providers` (vendors is deprecated)
    provider = "openai",
    providers = {
      openai = {
        -- Avante reads the API key from this env var
        api_key_name = "OPENAI_API_KEY",
        -- Set your desired model identifier
        model = "gpt-5",
        -- Force GPT-5 safe params (temp must be 1)
        extra_request_body = {
          temperature = 1,
        },
        -- Optional: override endpoint if needed
        -- endpoint = "https://api.openai.com/v1",
      },
      -- Optional experimental provider using OpenAI Responses API (non-streaming)
      -- Enable by setting `provider = "openai_responses"`
      openai_responses = {
        api_key_name = "OPENAI_API_KEY",
        endpoint = "https://api.openai.com/v1",
        model = "gpt-5",
        is_disable_stream = function()
          -- Use non-streaming for simpler integration with Responses API
          return true
        end,
        parse_curl_args = function(self, prompt_opts)
          local Providers = require("avante.providers")
          local provider_conf = Providers.parse_config(self)
          local headers = {
            ["Content-Type"] = "application/json",
            ["Authorization"] = "Bearer " .. (self.parse_api_key and self.parse_api_key() or os.getenv("OPENAI_API_KEY") or ""),
          }
          -- Collapse system prompt + messages into a single text input
          local function as_text_messages(messages)
            local chunks = {}
            for _, m in ipairs(messages or {}) do
              local content = m.content
              if type(content) == "table" then
                local buf = {}
                for _, part in ipairs(content) do
                  if type(part) == "string" then table.insert(buf, part)
                  elseif type(part) == "table" and (part.text or part.content) then
                    table.insert(buf, part.text or part.content)
                  end
                end
                content = table.concat(buf, "\n")
              end
              if type(content) == "string" and content ~= "" then
                table.insert(chunks, string.format("%s: %s", m.role or "user", content))
              end
            end
            return table.concat(chunks, "\n")
          end
          local input_text = (prompt_opts.system_prompt or "") .. "\n\n" .. as_text_messages(prompt_opts.messages or {})
          local body = {
            model = provider_conf.model or "gpt-5",
            input = input_text,
            -- Temperature must be 1 for GPT-5 per API error
            temperature = 1,
          }
          return {
            url = (provider_conf.endpoint or "https://api.openai.com/v1") .. "/responses",
            headers = headers,
            body = body,
            insecure = false,
          }
        end,
        parse_response_without_stream = function(data, _, opts)
          local ok, jsn = pcall(vim.json.decode, data)
          if not ok or not jsn then
            return opts.on_stop({ reason = "error", error = "Invalid JSON from Responses API" })
          end
          -- Prefer output_text; fallback to choices-like content if present
          local text = jsn.output_text
          if (not text or text == "") and jsn.output and jsn.output[1] and jsn.output[1].content then
            local buf = {}
            for _, part in ipairs(jsn.output[1].content) do
              if part.type == "output_text" and part.text then table.insert(buf, part.text) end
              if part.type == "text" and part.text then table.insert(buf, part.text) end
            end
            text = table.concat(buf, "")
          end
          if not text or text == "" then text = jsn.content or jsn.message or "" end
          if text ~= "" and opts.on_chunk then opts.on_chunk(text) end
          vim.schedule(function() opts.on_stop({ reason = "complete" }) end)
        end,
      },
    },
  },
}
